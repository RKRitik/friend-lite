name: Robot Framework Tests

on:
  pull_request:
    paths:
      - 'tests/**/*.robot'
      - 'tests/**/*.py'
      - 'backends/advanced/src/**'
      - '.github/workflows/robot-tests.yml'

permissions:
  contents: read
  pull-requests: write
  issues: write
  pages: write
  id-token: write

jobs:
  robot-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Verify required secrets
      env:
        DEEPGRAM_API_KEY: ${{ secrets.DEEPGRAM_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        echo "Verifying required secrets..."
        if [ -z "$DEEPGRAM_API_KEY" ]; then
          echo "âŒ ERROR: DEEPGRAM_API_KEY secret is not set"
          exit 1
        fi
        if [ -z "$OPENAI_API_KEY" ]; then
          echo "âŒ ERROR: OPENAI_API_KEY secret is not set"
          exit 1
        fi
        if [ -z "$HF_TOKEN" ]; then
          echo "âŒ ERROR: HF_TOKEN secret is not set"
          exit 1
        fi
        echo "âœ“ DEEPGRAM_API_KEY is set (length: ${#DEEPGRAM_API_KEY})"
        echo "âœ“ OPENAI_API_KEY is set (length: ${#OPENAI_API_KEY})"
        echo "âœ“ HF_TOKEN is set (length: ${#HF_TOKEN})"
        echo "âœ“ All required secrets verified"

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver-opts: |
          image=moby/buildkit:latest
          network=host

    - name: Cache Docker layers
      uses: actions/cache@v4
      with:
        path: /tmp/.buildx-cache
        key: ${{ runner.os }}-buildx-${{ hashFiles('backends/advanced/Dockerfile', 'backends/advanced/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-buildx-

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"

    - name: Install Robot Framework and dependencies
      run: |
        uv pip install --system robotframework robotframework-requests python-dotenv websockets

    - name: Create test config.yml
      run: |
        echo "Copying test configuration file..."
        mkdir -p config
        cp tests/configs/deepgram-openai.yml config/config.yml
        echo "âœ“ Test config.yml created from tests/configs/deepgram-openai.yml"
        ls -lh config/config.yml

    - name: Run Robot Framework tests
      working-directory: tests
      env:
        # Required for test runner script
        DEEPGRAM_API_KEY: ${{ secrets.DEEPGRAM_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        CLEANUP_CONTAINERS: "false"  # Don't cleanup in CI - handled by workflow
      run: |
        # Use the unified test script that mirrors local development
        ./run-robot-tests.sh
        TEST_EXIT_CODE=$?
        echo "test_exit_code=$TEST_EXIT_CODE" >> $GITHUB_ENV
        exit 0  # Don't fail here, we'll fail at the end after uploading artifacts

    - name: Show service logs
      if: always()
      working-directory: backends/advanced
      run: |
        echo "=== Backend Logs (last 50 lines) ==="
        docker compose -f docker-compose-test.yml logs --tail=50 chronicle-backend-test
        echo ""
        echo "=== Worker Logs (last 50 lines) ==="
        docker compose -f docker-compose-test.yml logs --tail=50 workers-test

    - name: Check if test results exist
      if: always()
      id: check_results
      run: |
        if [ -f tests/results/output.xml ]; then
          echo "results_exist=true" >> $GITHUB_OUTPUT
        else
          echo "results_exist=false" >> $GITHUB_OUTPUT
          echo "âš ï¸ No test results found in tests/results/"
          ls -la tests/results/ || echo "Results directory doesn't exist"
        fi

    - name: Upload Robot Framework HTML reports
      if: always() && steps.check_results.outputs.results_exist == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: robot-test-reports-html
        path: |
          tests/results/report.html
          tests/results/log.html
        retention-days: 30

    - name: Publish HTML Report as GitHub Pages artifact
      if: always() && steps.check_results.outputs.results_exist == 'true'
      uses: actions/upload-pages-artifact@v3
      with:
        path: tests/results

    - name: Deploy to GitHub Pages
      if: always() && steps.check_results.outputs.results_exist == 'true'
      uses: actions/deploy-pages@v4
      id: deployment

    - name: Generate test summary
      if: always() && steps.check_results.outputs.results_exist == 'true'
      id: test_summary
      run: |
        # Parse test results
        python3 << 'PYTHON_SCRIPT' > test_summary.txt
        import xml.etree.ElementTree as ET
        tree = ET.parse('tests/results/output.xml')
        root = tree.getroot()
        stats = root.find('.//total/stat')
        if stats is not None:
            passed = stats.get("pass", "0")
            failed = stats.get("fail", "0")
            total = int(passed) + int(failed)
            print(f"PASSED={passed}")
            print(f"FAILED={failed}")
            print(f"TOTAL={total}")
        PYTHON_SCRIPT

        # Source the variables
        source test_summary.txt

        # Set outputs
        echo "passed=$PASSED" >> $GITHUB_OUTPUT
        echo "failed=$FAILED" >> $GITHUB_OUTPUT
        echo "total=$TOTAL" >> $GITHUB_OUTPUT

    - name: Post PR comment with test results
      if: always() && steps.check_results.outputs.results_exist == 'true'
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const passed = '${{ steps.test_summary.outputs.passed }}';
          const failed = '${{ steps.test_summary.outputs.failed }}';
          const total = '${{ steps.test_summary.outputs.total }}';
          const runUrl = `https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}`;
          const pagesUrl = '${{ steps.deployment.outputs.page_url }}';

          const status = failed === '0' ? 'âœ… All tests passed!' : 'âŒ Some tests failed';
          const emoji = failed === '0' ? 'ğŸ‰' : 'âš ï¸';

          const comment = `## ${emoji} Robot Framework Test Results

          **Status**: ${status}

          | Metric | Count |
          |--------|-------|
          | âœ… Passed | ${passed} |
          | âŒ Failed | ${failed} |
          | ğŸ“Š Total | ${total} |

          ### ğŸ“Š View Reports

          **GitHub Pages (Live Reports):**
          - [ğŸ“‹ Test Report](${pagesUrl}report.html)
          - [ğŸ“ Detailed Log](${pagesUrl}log.html)

          **Download Artifacts:**
          - [robot-test-reports-html](${runUrl}) - HTML reports
          - [robot-test-results-xml](${runUrl}) - XML output

          ---
          *[View full workflow run](${runUrl})*`;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: Upload Robot Framework XML output
      if: always() && steps.check_results.outputs.results_exist == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: robot-test-results-xml
        path: tests/results/output.xml
        retention-days: 30

    - name: Upload logs on failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: robot-test-logs
        path: |
          backends/advanced/.env
          tests/setup/.env.test
        retention-days: 7

    - name: Display test results summary
      if: always()
      run: |
        if [ -f tests/results/output.xml ]; then
          echo "Test results generated successfully"
          echo "========================================"
          python3 << 'PYTHON_SCRIPT'
        import xml.etree.ElementTree as ET
        tree = ET.parse('tests/results/output.xml')
        root = tree.getroot()
        stats = root.find('.//total/stat')
        if stats is not None:
            passed = stats.get("pass", "0")
            failed = stats.get("fail", "0")
            print(f'âœ… Passed: {passed}')
            print(f'âŒ Failed: {failed}')
            print(f'ğŸ“Š Total: {int(passed) + int(failed)}')
        PYTHON_SCRIPT
          echo "========================================"
          echo ""
          echo "ğŸ“Š FULL TEST REPORTS AVAILABLE:"
          echo "  1. Go to the 'Summary' tab at the top of this page"
          echo "  2. Scroll down to 'Artifacts' section"
          echo "  3. Download 'robot-test-reports-html'"
          echo "  4. Extract and open report.html or log.html in your browser"
          echo ""
          echo "The HTML reports provide:"
          echo "  - report.html: Executive summary with statistics"
          echo "  - log.html: Detailed step-by-step execution log"
          echo ""
        fi

    - name: Cleanup
      if: always()
      working-directory: backends/advanced
      run: |
        docker compose -f docker-compose-test.yml down -v

    - name: Fail workflow if tests failed
      if: always()
      run: |
        if [ "${{ env.test_exit_code }}" != "0" ]; then
          echo "âŒ Tests failed with exit code ${{ env.test_exit_code }}"
          exit 1
        else
          echo "âœ… All tests passed"
        fi
